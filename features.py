# Imports
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots
from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler

# clustering
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering
from sklearn.decomposition import PCA
from sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score
from scipy.cluster.hierarchy import dendrogram, linkage

import warnings
warnings.filterwarnings('ignore')

# configuration de l'affichage
pd.set_option('display.max_columns', None)
pd.set_option('display.max_rows', 100)
plt.style.use('seaborn-v0_8-whitegrid')
sns.set_palette("husl")
plt.rcParams['figure.figsize'] = (12, 6)

def load_raw_data(data_path='data/fr-esr-parcoursup.csv', 
                  filiere_tres_agregee=None, 
                  filiere_detaillee=None):
    """
    Charge et filtre les donn√©es Parcoursup.
    
    Parameters:
    -----------
    data_path : str
        Chemin vers le fichier CSV
    filiere_tres_agregee : list[str], optional
        Liste des fili√®res tr√®s agr√©g√©es √† filtrer (ex: ["Ecole d'Ing√©nieur", "CPGE"])
        Par d√©faut: ["Ecole d'Ing√©nieur", "CPGE"]
    filiere_detaillee : list[str], optional
        Liste des fili√®res d√©taill√©es correspondantes (doit avoir la m√™me longueur que filiere_tres_agregee)
        Mettre None pour une fili√®re si on veut toutes les sous-fili√®res
        ex: [None, "Classe pr√©paratoire scientifique"] signifie:
            - Toutes les "Ecole d'Ing√©nieur"
            - Seulement les CPGE de type "Classe pr√©paratoire scientifique"
        Par d√©faut: [None, "Classe pr√©paratoire scientifique"]
    """
    # D√©finir les valeurs par d√©faut √† l'int√©rieur de la fonction (√©vite les probl√®mes de mutable default arguments)
    if filiere_tres_agregee is None:
        filiere_tres_agregee = ["Ecole d'Ing√©nieur", "CPGE"]
    if filiere_detaillee is None:
        filiere_detaillee = [None, "Classe pr√©paratoire scientifique"]
    
    # Chargement des donn√©es 
    df = pd.read_csv(data_path, sep=';', low_memory=False)
    
    # Construire le filtre dynamiquement
    print("\n" + "="*80)
    print("FILTRAGE DES DONN√âES")
    print("="*80)
    print(f"Fili√®res tr√®s agr√©g√©es: {filiere_tres_agregee}")
    print(f"Fili√®res d√©taill√©es: {filiere_detaillee}")
    
    if len(filiere_tres_agregee) != len(filiere_detaillee):
        raise ValueError("Les listes filiere_tres_agregee et filiere_detaillee doivent avoir la m√™me longueur")
    
    # Construire les conditions de filtrage
    conditions = []
    for filiere_main, filiere_sub in zip(filiere_tres_agregee, filiere_detaillee):
        if filiere_sub is None:
            # Pas de filtre sur la sous-fili√®re, prendre toutes les formations de cette fili√®re principale
            condition = (df['Fili√®re de formation tr√®s agr√©g√©e'] == filiere_main)
            print(f"  ‚úì Inclus: TOUTES les formations '{filiere_main}'")
        else:
            # Filtre sur la fili√®re principale ET la sous-fili√®re
            condition = (
                (df['Fili√®re de formation tr√®s agr√©g√©e'] == filiere_main) &
                (df['Fili√®re de formation.1'] == filiere_sub)
            )
            print(f"  ‚úì Inclus: '{filiere_main}' avec sous-fili√®re '{filiere_sub}'")
        conditions.append(condition)
    
    # Combiner toutes les conditions avec OR
    final_condition = conditions[0]
    for condition in conditions[1:]:
        final_condition = final_condition | condition
    
    df_inge = df[final_condition].copy()
    
    print(f"\n‚úì {len(df_inge)} formations filtr√©es sur {len(df)} total ({100*len(df_inge)/len(df):.2f}%)")
    print("="*80)

    FEATURES_KEYS = ["Code UAI de l'√©tablissement",
    "√âtablissement"]

    FEATURES_CLASSIFICATION = ["Statut de l‚Äô√©tablissement de la fili√®re de formation (public, priv√©‚Ä¶)",
    "D√©partement de l‚Äô√©tablissement",
    "R√©gion de l‚Äô√©tablissement", 
    "Commune de l‚Äô√©tablissement",
    "Coordonn√©es GPS de la formation",
    "Fili√®re de formation",
    "Fili√®re de formation tr√®s agr√©g√©e",
    "Fili√®re de formation d√©taill√©e bis",
    "Capacit√© de l‚Äô√©tablissement par formation",    
    "Effectif total des candidats en phase principale",
    "Rang du dernier appel√© du groupe 1"
]

    ## Calcul des fetures li√©es au genre
    f_ratio_candidats = df_inge["Dont effectif des candidates pour une formation"] / df_inge["Effectif total des candidats en phase principale"]
    f_ratio_admis = df_inge["% d‚Äôadmis dont filles"]/100

    f_selectivity_candidats = f_ratio_candidats / f_ratio_admis

    ## Calcul des features li√©es aux boursiers

    candidats_boursiers = (df_inge["Dont effectif des candidats boursiers n√©o bacheliers g√©n√©raux en phase principale"] +
    df_inge["Dont effectif des candidats boursiers n√©o bacheliers technologiques en phase principale"] +
    df_inge["Dont effectif des candidats boursiers n√©o bacheliers professionnels en phase principale"])


    b_ratio_candidats = candidats_boursiers / df_inge["Effectif total des candidats en phase principale"]
    b_ratio_admis = df_inge["% d‚Äôadmis n√©o bacheliers boursiers"]/100
    b_selectivity_candidats = b_ratio_candidats / b_ratio_admis

    ## Calcul de features li√©es √† l'origine (quel type bac)
    gen_candidats_ratio = df_inge["Effectif des candidats n√©o bacheliers g√©n√©raux en phase principale"] / df_inge["Effectif total des candidats en phase principale"]
    tech_candidats_ratio = df_inge["Effectif des candidats n√©o bacheliers technologiques en phase principale"] / df_inge["Effectif total des candidats en phase principale"]
    prof_candidats_ratio = df_inge["Effectif des candidats n√©o bacheliers professionnels en phase principale"] / df_inge["Effectif total des candidats en phase principale"]

    gen_admis_ratio = df_inge["% d‚Äôadmis n√©o bacheliers g√©n√©raux"]/100
    tech_admis_ratio = df_inge["% d‚Äôadmis n√©o bacheliers technologiques"]/100
    prof_admis_ratio = df_inge["% d‚Äôadmis n√©o bacheliers professionnels"]/100


    gen_selectivity_candidats = gen_candidats_ratio / gen_admis_ratio
    tech_selectivity_candidats = tech_candidats_ratio / tech_admis_ratio
    prof_selectivity_candidats = prof_candidats_ratio / prof_admis_ratio

    ## Calcul de fetaures li√©es √† la mention au bac
    sans_mention_ratio = df_inge["% d‚Äôadmis n√©o bacheliers sans mention au bac"]/100
    assez_bien_mention_ratio = df_inge["% d‚Äôadmis n√©o bacheliers avec mention Assez Bien au bac"]/100
    bien_mention_ratio = df_inge["% d‚Äôadmis n√©o bacheliers avec mention Bien au bac"]/100
    tres_bien_mention_ratio = df_inge["% d‚Äôadmis n√©o bacheliers avec mention Tr√®s Bien au bac"]/100
    tres_bien_avec_felicitation_mention_ratio = df_inge["% d‚Äôadmis n√©o bacheliers avec mention Tr√®s Bien avec f√©licitations au bac"]/100


    ## Calcul de features li√©es √† l'origine des candidats et admis

    meme_academie_ratio = df_inge["% d‚Äôadmis n√©o bacheliers issus de la m√™me acad√©mie"]/100
    meme_etablissement_ratio = df_inge["% d‚Äôadmis n√©o bacheliers issus du m√™me √©tablissement (BTS/CPGE)"]/100

    ## Calcul de features li√©es au rang du dernier appel√©, √† l'estimation de la part de refus, √† la selectivite (features "plus hautes")

    last_call_rank_ratio = (df_inge["Rang du dernier appel√© du groupe 1"] - df_inge["Capacit√© de l‚Äô√©tablissement par formation"])/ df_inge["Effectif total des candidats en phase principale"]
    pressure_ratio = df_inge["Capacit√© de l‚Äô√©tablissement par formation"]/df_inge["Effectif total des candidats en phase principale"]
    taux_acces_ratio = df_inge["Taux d‚Äôacc√®s"]/100


    # Cr√©ation du dataframe final avec les features s√©lectionn√©es et cr√©√©es
    df_final = pd.DataFrame()

    # Ajouter les features originales (CAPITAL)
    for feature in FEATURES_KEYS + FEATURES_CLASSIFICATION:
        df_final[feature] = df_inge[feature]

    # Ajouter les features cr√©√©es
    df_final['f_ratio_candidats'] = f_ratio_candidats
    df_final['f_ratio_admis'] = f_ratio_admis
    df_final['f_selectivity_candidats'] = f_selectivity_candidats

    df_final['b_ratio_candidats'] = b_ratio_candidats
    df_final['b_ratio_admis'] = b_ratio_admis
    df_final['b_selectivity_candidats'] = b_selectivity_candidats

    df_final['gen_candidats_ratio'] = gen_candidats_ratio
    df_final['tech_candidats_ratio'] = tech_candidats_ratio
    df_final['prof_candidats_ratio'] = prof_candidats_ratio

    df_final['gen_admis_ratio'] = gen_admis_ratio
    df_final['tech_admis_ratio'] = tech_admis_ratio
    df_final['prof_admis_ratio'] = prof_admis_ratio

    df_final['gen_selectivity_candidats'] = gen_selectivity_candidats
    df_final['tech_selectivity_candidats'] = tech_selectivity_candidats
    df_final['prof_selectivity_candidats'] = prof_selectivity_candidats

    df_final['sans_mention_ratio'] = sans_mention_ratio
    df_final['assez_bien_mention_ratio'] = assez_bien_mention_ratio
    df_final['bien_mention_ratio'] = bien_mention_ratio
    df_final['tres_bien_mention_ratio'] = tres_bien_mention_ratio
    df_final['tres_bien_avec_felicitation_mention_ratio'] = tres_bien_avec_felicitation_mention_ratio

    df_final['meme_academie_ratio'] = meme_academie_ratio
    df_final['meme_etablissement_ratio'] = meme_etablissement_ratio

    df_final['last_call_rank_ratio'] = last_call_rank_ratio
    df_final['pressure_ratio'] = pressure_ratio
    df_final['taux_acces_ratio'] = taux_acces_ratio

    # R√©initialiser l'index pour avoir un dataframe propre
    df_final = df_final.reset_index(drop=True)

    print(f"DataFrame final cr√©√© avec {len(df_final)} lignes et {len(df_final.columns)} colonnes")
    print(f"\nColonnes incluses:")
    print(df_final.columns.tolist())

    df_final.to_csv("data/df_features.csv", index=False)
    return df_final


def clean_data(df_final, alpha_geographique=0.98):

    # Nettoyage des donn√©es g√©ographiques
    print("\n" + "="*80)
    print("NETTOYAGE DES COORDONN√âES GPS")
    print("="*80)
    
    # Extraire longitude et latitude
    df_final["longitude"] = df_final["Coordonn√©es GPS de la formation"].str.split(",").str[0]
    df_final["latitude"] = df_final["Coordonn√©es GPS de la formation"].str.split(",").str[1]

    df_final["longitude"] = df_final["longitude"].astype(float)
    df_final["latitude"] = df_final["latitude"].astype(float)
    
    # Statistiques avant clipping
    print("\nStatistiques des coordonn√©es AVANT clipping:")
    print(f"  Longitude: min={df_final['longitude'].min():.4f}, max={df_final['longitude'].max():.4f}, "
          f"m√©diane={df_final['longitude'].median():.4f}")
    print(f"  Latitude: min={df_final['latitude'].min():.4f}, max={df_final['latitude'].max():.4f}, "
          f"m√©diane={df_final['latitude'].median():.4f}")
    
    # Clipping des valeurs extr√™mes (Winsorization) - comprime les outliers
    # Utilise les percentiles 1 et 99 pour garder 98% des donn√©es intactes
    percentile_low = (1 - alpha_geographique) * 100  # 1st percentile
    percentile_high = alpha_geographique * 100  # 99th percentile
    
    lon_low = df_final["longitude"].quantile(percentile_low / 100)
    lon_high = df_final["longitude"].quantile(percentile_high / 100)
    lat_low = df_final["latitude"].quantile(percentile_low / 100)
    lat_high = df_final["latitude"].quantile(percentile_high / 100)
    
    # Compter les valeurs qui seront clipp√©es
    n_outliers_lon = ((df_final["longitude"] < lon_low) | (df_final["longitude"] > lon_high)).sum()
    n_outliers_lat = ((df_final["latitude"] < lat_low) | (df_final["latitude"] > lat_high)).sum()
    
    print(f"\nClipping des valeurs extr√™mes (percentiles {percentile_low}% - {percentile_high}%):")
    print(f"  Longitude: [{lon_low:.4f}, {lon_high:.4f}]")
    print(f"  Latitude: [{lat_low:.4f}, {lat_high:.4f}]")
    print(f"  Valeurs clipp√©es: {n_outliers_lon} longitudes, {n_outliers_lat} latitudes")
    
    # Clipper les valeurs extr√™mes
    df_final["longitude"] = df_final["longitude"].clip(lower=lon_low, upper=lon_high)
    df_final["latitude"] = df_final["latitude"].clip(lower=lat_low, upper=lat_high)
    
    # Statistiques apr√®s clipping
    print("\nStatistiques des coordonn√©es APR√àS clipping:")
    print(f"  Longitude: min={df_final['longitude'].min():.4f}, max={df_final['longitude'].max():.4f}")
    print(f"  Latitude: min={df_final['latitude'].min():.4f}, max={df_final['latitude'].max():.4f}")
    
    # Normalisation MinMax apr√®s clipping
    scaler = MinMaxScaler()
    df_final[["longitude", "latitude"]] = scaler.fit_transform(df_final[["longitude", "latitude"]])
    
    print("\nNormalisation MinMax appliqu√©e (valeurs entre 0 et 1)")
    print(f"  Longitude normalis√©e: min={df_final['longitude'].min():.4f}, max={df_final['longitude'].max():.4f}")
    print(f"  Latitude normalis√©e: min={df_final['latitude'].min():.4f}, max={df_final['latitude'].max():.4f}")
    print("="*80)
    
    print("\n" + "="*80)
    print("NETTOYAGE DES DONN√âES AVANT NORMALISATION")
    print("="*80)

    # Identifier et g√©rer les valeurs probl√©matiques (inf, -inf, NaN)
    numeric_cols_check = df_final.select_dtypes(include=[np.number]).columns.tolist()

    print(f"\nV√©rification des valeurs probl√©matiques dans {len(numeric_cols_check)} colonnes num√©riques:")
    has_issues = False

    for col in numeric_cols_check:
        n_inf = np.isinf(df_final[col]).sum()
        n_nan = df_final[col].isna().sum()
        if n_inf > 0 or n_nan > 0:
            print(f"  ‚ö†Ô∏è  {col}: {n_inf} inf, {n_nan} NaN")
            has_issues = True

    if not has_issues:
        print("  ‚úì Aucune valeur probl√©matique d√©tect√©e")

    # Remplacer les valeurs infinies et NaN
    print("\nTraitement des valeurs probl√©matiques:")
    print("  - inf ‚Üí remplac√© par 1")
    print("  - -inf ‚Üí remplac√© par -1")
    print("  - NaN ‚Üí remplac√© par la m√©diane de la colonne")

    df_final_cleaned = df_final.copy()

    for col in numeric_cols_check:
        # Remplacer inf par 1 et -inf par -1
        df_final_cleaned[col] = df_final_cleaned[col].replace([np.inf, -np.inf], [1, -1])
        
        # Remplacer NaN par la m√©diane (plus robuste que la moyenne pour les outliers)
        if df_final_cleaned[col].isna().sum() > 0:
            median_val = df_final_cleaned[col].median()
            # Si la m√©diane est aussi NaN (toute la colonne est NaN), utiliser 0
            if pd.isna(median_val):
                median_val = 0
            df_final_cleaned[col] = df_final_cleaned[col].fillna(median_val)
            print(f"  ‚úì {col}: rempli avec m√©diane = {median_val:.4f}")

    # V√©rification finale
    n_inf_total = np.isinf(df_final_cleaned[numeric_cols_check]).sum().sum()
    n_nan_total = df_final_cleaned[numeric_cols_check].isna().sum().sum()
    print(f"\n‚úì Apr√®s nettoyage: {n_inf_total} inf, {n_nan_total} NaN")

    # Sauvegarder le dataframe nettoy√©
    df_final_cleaned.to_csv("data/df_features_cleaned.csv", index=False)
    print(f"‚úì DataFrame nettoy√© sauvegard√© dans 'data/df_features_cleaned.csv'")
    return df_final_cleaned

# =============================================================================
# NORMALISATION DES FEATURES POUR LE CLUSTERING
# =============================================================================

def normalize_features_for_clustering(df, exclude_cols=None, method='minmax_symmetric'):
    """
    Normalise les features num√©riques d'un dataframe pour le clustering.
    
    Parameters:
    -----------
    df : pd.DataFrame
        Le dataframe √† normaliser
    exclude_cols : list, optional
        Liste des colonnes √† exclure de la normalisation (par exemple les identifiants)
    method : str, default='minmax_symmetric'
        M√©thode de normalisation:
        - 'minmax_symmetric': MinMaxScaler vers [-1, +1] - recommand√© pour avoir toutes les colonnes sur la m√™me √©chelle
        - 'standard': StandardScaler (z-score)
        - 'minmax': MinMaxScaler (0-1)
        - 'robust': RobustScaler (m√©diane et IQR, r√©sistant aux outliers)
    
    Returns:
    --------
    df_normalized : pd.DataFrame
        Dataframe avec les colonnes num√©riques normalis√©es
    scaler : Scaler object ou None
        L'objet scaler utilis√© (None pour minmax_symmetric car normalisation manuelle)
    numeric_cols : list
        Liste des colonnes num√©riques qui ont √©t√© normalis√©es
    """
    
    
    df_normalized = df.copy()
    
    # Identifier les colonnes num√©riques
    numeric_cols = df_normalized.select_dtypes(include=[np.number]).columns.tolist()
    
    # Exclure certaines colonnes si sp√©cifi√©
    if exclude_cols:
        numeric_cols = [col for col in numeric_cols if col not in exclude_cols]
    
    # Choisir le scaler
    if method == 'minmax_symmetric':
        print("üìä Utilisation de MinMaxScaler sym√©trique ([-1, +1] normalization)")
        print("   Formule: 2 * (X - min) / (max - min) - 1")
        
        # Normalisation manuelle vers [-1, +1] pour chaque colonne
        for col in numeric_cols:
            min_val = df_normalized[col].min()
            max_val = df_normalized[col].max()
            
            if max_val - min_val != 0:
                # Normaliser vers [-1, +1]
                df_normalized[col] = 2 * (df_normalized[col] - min_val) / (max_val - min_val) - 1
            else:
                # Si la colonne est constante, mettre √† 0
                df_normalized[col] = 0
        
        scaler = None  # Pas de scaler sklearn pour cette m√©thode custom
        
    elif method == 'standard':
        scaler = StandardScaler()
        print("üìä Utilisation de StandardScaler (z-score normalization)")
        df_normalized[numeric_cols] = scaler.fit_transform(df_normalized[numeric_cols])
        
    elif method == 'minmax':
        scaler = MinMaxScaler()
        print("üìä Utilisation de MinMaxScaler (0-1 normalization)")
        df_normalized[numeric_cols] = scaler.fit_transform(df_normalized[numeric_cols])
        
    elif method == 'robust':
        scaler = RobustScaler()
        print("üìä Utilisation de RobustScaler (robust to outliers)")
        df_normalized[numeric_cols] = scaler.fit_transform(df_normalized[numeric_cols])
    else:
        raise ValueError(f"M√©thode inconnue: {method}")
    
    print(f"‚úì {len(numeric_cols)} colonnes num√©riques normalis√©es")
    
    # V√©rification des min/max pour minmax_symmetric
    if method == 'minmax_symmetric':
        print("\nüìà V√©rification des valeurs normalis√©es:")
        for col in numeric_cols[:5]:  # Afficher les 5 premi√®res colonnes
            print(f"   {col}: min={df_normalized[col].min():.4f}, max={df_normalized[col].max():.4f}")
        if len(numeric_cols) > 5:
            print(f"   ... ({len(numeric_cols) - 5} autres colonnes)")
    
    return df_normalized, scaler, numeric_cols


def visualize_normalization_impact(df_original, df_normalized, numeric_cols, save_prefix='data/normalization'):
    """
    Visualise l'impact de la normalisation sur les features.
    
    Parameters:
    -----------
    df_original : pd.DataFrame
        Dataframe avant normalisation
    df_normalized : pd.DataFrame
        Dataframe apr√®s normalisation
    numeric_cols : list
        Liste des colonnes num√©riques normalis√©es
    save_prefix : str
        Pr√©fixe pour sauvegarder les graphiques
    """
    
    # Filtrer uniquement les colonnes *_ratio pour la visualisation
    ratio_cols = [col for col in numeric_cols if col.endswith('_ratio')]
    
    print(f"üìä Visualisation de {len(ratio_cols)} features *_ratio (sur {len(numeric_cols)} features totales)")
    print(f"   Features visualis√©es: {', '.join(ratio_cols)}")
    
    n_features = len(ratio_cols)
    
    # =========================================================================
    # VISUALISATION 1: Comparaison avant/apr√®s pour quelques features
    # =========================================================================
    n_samples = min(8, n_features)
    sample_features = ratio_cols[:n_samples]
    
    fig, axes = plt.subplots(n_samples, 2, figsize=(14, 4*n_samples))
    if n_samples == 1:
        axes = axes.reshape(1, -1)
    
    for i, col in enumerate(sample_features):
        # Avant normalisation
        axes[i, 0].hist(df_original[col].dropna(), bins=50, color='steelblue', alpha=0.7, edgecolor='black')
        axes[i, 0].set_title(f'AVANT: {col}', fontsize=11, fontweight='bold')
        axes[i, 0].set_xlabel('Valeur', fontsize=9)
        axes[i, 0].set_ylabel('Fr√©quence', fontsize=9)
        axes[i, 0].grid(True, alpha=0.3)
        
        # Apr√®s normalisation
        axes[i, 1].hist(df_normalized[col].dropna(), bins=50, color='coral', alpha=0.7, edgecolor='black')
        axes[i, 1].set_title(f'APR√àS: {col}', fontsize=11, fontweight='bold')
        axes[i, 1].set_xlabel('Valeur normalis√©e', fontsize=9)
        axes[i, 1].set_ylabel('Fr√©quence', fontsize=9)
        axes[i, 1].grid(True, alpha=0.3)
    
    plt.suptitle('Comparaison des distributions AVANT et APR√àS normalisation (√©chantillon)', 
                 fontsize=14, fontweight='bold', y=1.001)
    plt.tight_layout()
    plt.savefig(f'{save_prefix}_sample_comparison.png', dpi=300, bbox_inches='tight')
    print(f"‚úì Graphique '{save_prefix}_sample_comparison.png' sauvegard√©")
    plt.show()
    
    # =========================================================================
    # VISUALISATION 2: Box plots comparatifs pour toutes les features *_ratio
    # =========================================================================
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, max(10, n_features * 0.35)))
    
    # Avant normalisation
    positions = np.arange(len(ratio_cols))
    bp1 = ax1.boxplot([df_original[col].dropna() for col in ratio_cols], 
                       vert=False, patch_artist=True, positions=positions, widths=0.6)
    
    colors = plt.cm.Blues(np.linspace(0.4, 0.8, len(ratio_cols)))
    for patch, color in zip(bp1['boxes'], colors):
        patch.set_facecolor(color)
        patch.set_alpha(0.7)
    
    ax1.set_yticks(positions)
    ax1.set_yticklabels(ratio_cols, fontsize=8)
    ax1.set_xlabel('Valeur originale', fontsize=10, fontweight='bold')
    ax1.set_title('AVANT normalisation', fontsize=12, fontweight='bold')
    ax1.grid(True, alpha=0.3, axis='x')
    
    # Apr√®s normalisation
    bp2 = ax2.boxplot([df_normalized[col].dropna() for col in ratio_cols], 
                       vert=False, patch_artist=True, positions=positions, widths=0.6)
    
    colors = plt.cm.Oranges(np.linspace(0.4, 0.8, len(ratio_cols)))
    for patch, color in zip(bp2['boxes'], colors):
        patch.set_facecolor(color)
        patch.set_alpha(0.7)
    
    ax2.set_yticks(positions)
    ax2.set_yticklabels(ratio_cols, fontsize=8)
    ax2.set_xlabel('Valeur normalis√©e [-1, +1]', fontsize=10, fontweight='bold')
    ax2.set_title('APR√àS normalisation', fontsize=12, fontweight='bold')
    ax2.grid(True, alpha=0.3, axis='x')
    ax2.axvline(x=-1, color='red', linestyle='--', alpha=0.5, linewidth=1)
    ax2.axvline(x=0, color='gray', linestyle='--', alpha=0.5, linewidth=1)
    ax2.axvline(x=1, color='red', linestyle='--', alpha=0.5, linewidth=1)
    
    plt.suptitle('Distribution des features *_ratio AVANT et APR√àS normalisation [-1, +1]', 
                 fontsize=14, fontweight='bold', y=0.998)
    plt.tight_layout()
    plt.savefig(f'{save_prefix}_all_boxplots.png', dpi=300, bbox_inches='tight')
    print(f"‚úì Graphique '{save_prefix}_all_boxplots.png' sauvegard√©")
    plt.show()
    
    # =========================================================================
    # R√âSUM√â STATISTIQUE
    # =========================================================================
    print("\n" + "="*80)
    print("R√âSUM√â DE LA NORMALISATION")
    print("="*80)
    
    print(f"\nNombre de features normalis√©es: {len(numeric_cols)}")
    print(f"  dont {len(ratio_cols)} features *_ratio visualis√©es")
    
    print(f"\nStatistiques des features *_ratio AVANT normalisation:")
    for col in ratio_cols:
        print(f"  {col}: min={df_original[col].min():.4f}, max={df_original[col].max():.4f}, mean={df_original[col].mean():.4f}")
    
    print(f"\nStatistiques des features *_ratio APR√àS normalisation:")
    for col in ratio_cols:
        print(f"  {col}: min={df_normalized[col].min():.4f}, max={df_normalized[col].max():.4f}, mean={df_normalized[col].mean():.4f}")
    
    print("\n‚úì Toutes les colonnes sont maintenant sur l'√©chelle [-1, +1]")
    print("="*80)



def prepare_categorical_features_for_clustering(df):
    """
    Encode les features cat√©gorielles pour le clustering.
    
    Strat√©gie:
    - Supprime: Code UAI, √âtablissement, Coordonn√©es GPS, Commune
    - One-hot encode (normalis√© vers [-1, +1]): Statut, Fili√®re, R√©gion, D√©partement
    
    Returns: df_encoded, n_new_features
    """
    df_encoded = df.copy()
    
    # Colonnes √† supprimer (identifiants et haute cardinalit√©)
    cols_to_remove = [
        "Code UAI de l'√©tablissement",
        "√âtablissement",
        "Coordonn√©es GPS de la formation",
        "Commune de l'√©tablissement"
    ]
    
    for col in cols_to_remove:
        if col in df_encoded.columns:
            df_encoded = df_encoded.drop(columns=[col])
    
    # Colonnes √† one-hot encoder
    cols_to_encode = [
        "Statut de l'√©tablissement de la fili√®re de formation (public, priv√©‚Ä¶)",
        "Fili√®re de formation",
        "Fili√®re de formation tr√®s agr√©g√©e",
        "Fili√®re de formation d√©taill√©e bis",
        "D√©partement de l'√©tablissement",
        "R√©gion de l'√©tablissement"
    ]
    
    n_new_features = 0
    for col in cols_to_encode:
        if col in df_encoded.columns:
            # One-hot encoding
            dummies = pd.get_dummies(df_encoded[col], prefix=col[:25], drop_first=False)
            # Normaliser vers [-1, 1]: 0 -> -1, 1 -> 1
            dummies = dummies * 2 - 1
            df_encoded = pd.concat([df_encoded, dummies], axis=1)
            df_encoded = df_encoded.drop(columns=[col])
            n_new_features += len(dummies.columns)
    
    # Supprimer toute colonne cat√©gorielle restante
    remaining_cat = df_encoded.select_dtypes(exclude=['number']).columns.tolist()
    if remaining_cat:
        df_encoded = df_encoded.drop(columns=remaining_cat)
    
    return df_encoded, n_new_features



# =============================================================================
# CODE PRINCIPAL - Ne s'ex√©cute que si ce fichier est lanc√© directement
# =============================================================================
if __name__ == "__main__":
    df_final = load_raw_data()


    # =============================================================================
    # APPLICATION DE LA NORMALISATION ET DU CLEANING
    # =============================================================================

    # Exclure les colonnes cat√©gorielles et garder uniquement les num√©riques
    categorical_cols = ["Code UAI de l'√©tablissement", 
                       "√âtablissement",
                       "Statut de l'√©tablissement de la fili√®re de formation (public, priv√©‚Ä¶)",
                       "D√©partement de l'√©tablissement",
                       "R√©gion de l'√©tablissement",
                       "Commune de l'√©tablissement",
                       "Coordonn√©es GPS de la formation",
                       "Fili√®re de formation",
                       "Fili√®re de formation tr√®s agr√©g√©e",
                       "Fili√®re de formation d√©taill√©e bis"]



    print("\n" + "="*80)
    print("NORMALISATION DES FEATURES POUR LE CLUSTERING")
    print("="*80)


    df_final_cleaned = clean_data(df_final)
    # Appliquer la normalisation MinMax sym√©trique [-1, +1] sur chaque colonne
    df_normalized, scaler, numeric_cols_normalized = normalize_features_for_clustering(
        df_final_cleaned, 
        exclude_cols=categorical_cols,
        method='robust'
    )

    # Sauvegarder le dataframe normalis√©
    df_normalized.to_csv("data/df_features_normalized.csv", index=False)
    print(f"‚úì DataFrame normalis√© sauvegard√© dans 'data/df_features_normalized.csv'")

    # # Visualiser l'impact de la normalisation

    # visualize_normalization_impact(df_final_cleaned, df_normalized, numeric_cols_normalized)


    # =============================================================================
    # INT√âGRATION DES FEATURES CAT√âGORIELLES POUR LE CLUSTERING
    # =============================================================================


    # =============================================================================
    # APPLICATION DE L'ENCODAGE CAT√âGORIEL
    # =============================================================================

    # print("\n" + "="*80)
    # print("ENCODAGE DES FEATURES CAT√âGORIELLES")
    # print("="*80)

    # df_clustering, n_new = prepare_categorical_features_for_clustering(df_normalized)

    # print(f"\n‚úì Encodage termin√©:")
    # print(f"  - {n_new} nouvelles features cr√©√©es")
    # print(f"  - {len(df_clustering.columns)} features totales")
    # print(f"  - {len(df_clustering)} lignes")

    # # Sauvegarder
    # df_clustering.to_csv("data/df_fetures_normalized_categorical.csv", index=False)



